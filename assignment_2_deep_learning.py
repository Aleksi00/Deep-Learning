# -*- coding: utf-8 -*-
"""Assignment 2 Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XrfCsX2X_sycV-y1NE2aIeK3UsKXr4_S
"""

# Deep Learning Assignment 2
# Aleksi Sormunen 12310420 and Luisa Weisch 00473206

# Imports:
import pickle
import numpy as np
import sklearn
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn import preprocessing
from tensorflow import keras
from keras import layers
from keras.callbacks import LearningRateScheduler
import tensorflow as tf
import matplotlib.pyplot as plt

# Load Dataset
dict = pickle.load(open("california-housing-dataset.pkl", "rb"))
x_train, y_train = dict["x_train"], dict["y_train"]
x_test, y_test = dict["x_test"], dict["y_test"]

# a)
# Construct validation set from the training set and the test set is kept separate
x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25)

# Normalize data
scaler = StandardScaler()
x_train_norm = scaler.fit_transform(x_train)
x_valid_norm = scaler.transform(x_valid)
x_test_norm = scaler.transform(x_test)

# Gettng familiar with dataset
print(f" Shape of x Training set:{x_train.shape}")
print(f" Shape of x Training set:{x_valid.shape}")

num_features = x_train.shape[1]
num_rows = int(np.ceil(num_features / 3))
fig, axs = plt.subplots(num_rows, 3, figsize=(10, 3 * num_rows))

feature_names = [
    'MedInc',
    'HouseAge',
    'AveRooms',
    'AveBedrms',
    'Population',
    'AveOccup',
    'Latitude',
    'Longitude'
]
# investigating the distributions of features
for i in range(num_features):
    row = i // 3
    col = i % 3
    axs[row, col].hist(x_train[:, i], bins=50)
    axs[row, col].set_title(f'Distribution of {feature_names[i]}')
    axs[row, col].set_xlabel('Value')
    axs[row, col].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Task b)

# Parameters
num_epochs = 50
loss_function = 'mean_squared_error'
optimizer = 'adam'

# Testing different Architectures for Model
h_layers = [2,3]
h_units = [32,64,128]
batch_sizes = [32,64]


df = pd.DataFrame(columns=['Number of Hidden Layers', 'Number of Hidden Units', 'Batch Size', 'Loss', 'Validation Loss'])
df.style.set_caption("Loss Overview for different Model Architectures")

for num_layers in h_layers:
  for num_units in h_units:

    if num_layers == 1:
      model = keras.Sequential([
          layers.Input(shape=(len(x_test_norm[1]),)),  # Input layer
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(1, activation ='linear')])      # Output layer

    if num_layers == 2:
      model = keras.Sequential([
          layers.Input(shape=(len(x_test_norm[1]),)),  # Input layer
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),
          layers.Dense(1, activation ='linear')])      # Output layer

    if num_layers == 3:
      model = keras.Sequential([
          layers.Input(shape=(len(x_test_norm[1]),)),  # Input layer
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),
          layers.Dense(num_units, activation='relu'),
          layers.Dense(1, activation ='linear')])      # Output layer

    for batch in batch_sizes:
      model.compile(optimizer=optimizer, loss=loss_function)
      history = model.fit(x_train_norm, y_train, batch_size=batch, epochs=num_epochs, validation_data=(x_valid_norm, y_valid))

      training_loss = history.history['loss']
      validation_loss = history.history['val_loss']

      new_row = {'Number of Hidden Layers': num_layers, 'Number of Hidden Units': num_units,
                'Batch Size' : batch,
                'Loss': training_loss[-1], 'Validation Loss': validation_loss[-1]}

      df.loc[len(df)] = new_row

df

# c)
# Optimizer and Learning rate
# Parameters
batch_size = 64
num_epochs = 25
num_units = 128
loss_function = 'mean_squared_error'
learning_rates = [0.01, 0.001, 0.0001]

# Data frame for comparing:
df_lr = pd.DataFrame(columns=['Optimizer', 'learning rate', 'Training Loss', 'Validation Loss'])

# Defining Model Architecture
model = keras.Sequential([
          layers.Input(shape=(len(x_test[1]),)),       # Input layer
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 2 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 3 with ReLU activation
          layers.Dense(1, activation ='linear')])      # Output layer

# Optimizer 1: ADAM
optimizer = keras.optimizers.Adam(learning_rate=0.01)

for new_rate in learning_rates:
  optimizer = keras.optimizers.Adam(learning_rate=new_rate)
  model.compile(optimizer=optimizer, loss=loss_function)
  history = model.fit(x_train_norm, y_train, batch_size=batch, epochs=num_epochs, validation_data=(x_valid_norm, y_valid))

  training_loss = history.history['loss']
  validation_loss = history.history['val_loss']


  new_row = {'Optimizer': 'ADAM', 'learning rate': new_rate,
                  'Training Loss': training_loss[-1], 'Validation Loss': validation_loss[-1]}

  df_lr.loc[len(df_lr)] = new_row

# Optimizer 2: SGD
optimizer = keras.optimizers.experimental.SGD(learning_rate=0.001)
for new_rate in learning_rates:
  optimizer = keras.optimizers.experimental.SGD(learning_rate=new_rate)
  model.compile(optimizer=optimizer, loss=loss_function)
  history = model.fit(x_train_norm, y_train, batch_size=batch, epochs=num_epochs, validation_data=(x_valid_norm, y_valid))

  training_loss = history.history['loss']
  validation_loss = history.history['val_loss']

  new_row = {'Optimizer': 'SGD', 'learning rate': new_rate,
                  'Training Loss': training_loss[-1], 'Validation Loss': validation_loss[-1]}

  df_lr.loc[len(df_lr)] = new_row

# Optimizer 3: momentum SGD
optimizer = keras.optimizers.experimental.SGD(learning_rate=0.001)
for new_rate in learning_rates:
  optimizer = keras.optimizers.experimental.SGD(learning_rate=new_rate)
  model.compile(optimizer=optimizer, loss=loss_function)
  history = model.fit(x_train_norm, y_train, batch_size=batch, epochs=num_epochs, validation_data=(x_valid_norm, y_valid))

  training_loss = history.history['loss']
  validation_loss = history.history['val_loss']

  new_row = {'Optimizer': 'momentum SGD', 'learning rate': new_rate,
             'Training Loss': training_loss[-1], 'Validation Loss': validation_loss[-1]}

  df_lr.loc[len(df_lr)] = new_row

df_lr

# Task d)
# Parameters for Architecture
batch = 64
num_epochs = 50
num_units = 128
optimizer = keras.optimizers.experimental.SGD(learning_rate = 0.0001, momentum = 0.9)
loss_function = 'mean_squared_error'

# Defining Model Architecture
model = keras.Sequential([
          layers.Input(shape=(len(x_test[1]),)),  # Input layer
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(num_units, activation='relu'),  # Hidden layer 1 with ReLU activation
          layers.Dense(1, activation ='linear')])  # Output layer


model.compile(optimizer=optimizer, loss=loss_function)
history = model.fit(x_train_norm, y_train, batch_size=batch, epochs=num_epochs, validation_data=(x_valid_norm, y_valid))

# Plot Training Loss and Validation Loss
# Variables
x_batch = list(range(1,len(training_loss)+1, 1))
training_loss = history.history['loss']
validation_loss = history.history['val_loss']

# Plots
plt.plot(training_loss, label = 'Training Loss')
plt.plot(validation_loss, label = 'Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Evolution of Training and Validation Errors during Training')
plt.legend()
plt.show()

# Prediction vs. True Values Scatter Plot
final_test_loss = model.evaluate(x_test_norm, y_test, verbose=0)
print(f"Final Test Loss: {final_test_loss}")

prediction = model.predict(x_test_norm)

# Scatter Plot
plt.scatter(y_test, prediction.flatten(), s = 0.2)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Comparison of Predicted and True Values of Test Set')
plt.plot([0,5], [0,5], '--r')
plt.show

# e) Binary Classification

# First steps again, so the redifinition of the targets are using the correct dataset
# Load Dataset
dict = pickle.load(open("california-housing-dataset.pkl", "rb"))
x_train, y_train = dict["x_train"], dict["y_train"]
x_test, y_test = dict["x_test"], dict["y_test"]


# Defining Model Architecture
binary_model = keras.Sequential()
binary_model.add(keras.layers.Dense(64, activation='relu', input_shape=(8,)))
binary_model.add(keras.layers.Dense(64, activation='relu'))
binary_model.add(keras.layers.Dense(1, activation='sigmoid'))  # Sigmoid for binary classification
binary_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Redefine target variables for binary classification
y_train[y_train<2], y_test[y_test<2] = 0, 0
y_train[y_train>=2], y_test[y_test>=2]  = 1, 1

# Train the model on the training set
binary_history = binary_model.fit(x_train, y_train, epochs=50, batch_size=32)

# Evaluate the model on the test set
binary_test_loss = binary_model.evaluate(x_test, y_test)
print(f"Binary Classification Test Loss: {binary_test_loss[0]} and Accuracy: {binary_test_loss[1]}")

