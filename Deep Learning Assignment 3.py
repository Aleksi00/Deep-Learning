# -*- coding: utf-8 -*-
"""DL_A3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1geRy47esArSKpvSHDLYi_zeLBg18Dcy2
"""

import tensorflow as tf
import tensorflow.keras.datasets as tfd
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Dropout
from tensorflow.keras import regularizers
import pickle
import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix
import seaborn as sns

"""# Assignment 3
Aleksi Sormunen 12310420 and Luisa Weisch 00473206
"""

# Grayscaled CIFAR-20
import tensorflow.keras.datasets as tfd
train_data, test_data = tfd.cifar100.load_data(label_mode="coarse")
(x_train, y_train), (x_test, y_test) = train_data, test_data
x_train, x_test = np.mean(x_train, axis=3), np.mean(x_test, axis=3)

# perteurbed test set
# import pickle
# dict = pickle.load(open(‘cifar20_perturb_test.pkl’, ‘rb’))
# x_perturb, y_perturb = dict[‘x_perturb’], dict[‘y_perturb’]
# x_perturb = np.mean(x_perturb, axis=3)

"""Looking for: construct CNN (Convolutional Neural Network)

high prediction perform on x_test but also on x_perturb

## a)
 Load and prepare your data and labels.\
 Familiarize yourself with the dataset and problem. \
 Construct a stratified validation set consisting of samples from the training data, which will be used during the model selection process.\
 You will use the test set only for final evaluations.
"""

# a) Load and prepare data
train_data, test_data = tfd.cifar100.load_data(label_mode="coarse")
(x_train, y_train), (x_test, y_test) = train_data, test_data
x_train, x_test = np.mean(x_train, axis=3), np.mean(x_test, axis=3)

# Validation set:
x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.5)

# Dataset
total = np.shape(x_train)[0] + np.shape(x_test)[0] + np.shape(x_valid)[0]
print(f'Shape x_train: {np.shape(x_train)} and y_train: {np.shape(y_train)}')
print('x samples are 32x32 pixles pictures, y samples classification')

#np.shape(x_test)[0] + np.shape(x_valid)[0]
print(f'Total samples: {total}')
print(f'Splitting into {round(np.shape(x_train)[0]/total * 100)}% test samples, ',
      f'{round(np.shape(x_test)[0]/total * 100)}% training samples, '
      f'{round(np.shape(x_test)[0]/total * 100)}%')

# compare test and train figures
i_list = []
j_list = []
f, axarr = plt.subplots(19,2, figsize = (10,50))

for label in range(0,19):
  i = 0
  while i < len(y_train):
    if y_train[i] == label:
      axarr[label,0].imshow(x_train[i,:,:], cmap='gray')
      axarr[label,0].set_title("x_train class {}".format(y_train[i]))
      axarr[label,0].axis('off')
      i_list.append(i)
      break
    i += 1
  j = 0
  while  j < len(y_test):
    if y_test[j] == label:
      axarr[label,1].imshow(x_test[j,:,:], cmap='gray')
      axarr[label,1].set_title("x_ test class {}".format(y_test[j]))
      axarr[label,1].axis('off')
      j_list.append(j)
      break
    j += 1

"""## b)
Design and train your customized CNN for this multi-class classification task, that achieves good performance on the grayscale CIFAR-20 test set.\
Explain your choices for the output layer and the error function that you will use.\
During training with mini-batches, use early stopping based on the
validation set to avoid overfitting.\
Test different architectures with varying numbers of convolutional layers and number of kernels (i.e.,change the architecture in depth and width).\
Compare training and validation set errors and accuracies of (at least five) different architectures and report in a table.
"""

# b)
# Normalize data - Pixel intensity
x_train_norm = x_train/255
x_test_norm = x_test/255
x_valid_norm = x_valid/255

# Create CNN Model
k = 2
layer_size = [2,3]
batch_size = [4,8,16]

df = pd.DataFrame(columns=['Number of Layers', 'Batch Size', 'Kernel Size', 'Training Accuracy', 'Validation Accuracy'])
df.style.set_caption("Accuracy Overview for different Model Architectures")


for layer in layer_size:
  model1 = models.Sequential()
  model1.add(layers.Conv2D(32, (k,k), activation='relu', input_shape = (32,32, 1)))
  if layer == 2:
    model1.add(layers.MaxPooling2D((2,2)))
    model1.add(layers.Conv2D(64,(k,k), activation='relu'))
    model1.add(layers.MaxPooling2D((2,2)))
    model1.add(layers.Conv2D(64,(k,k), activation='relu'))
  if layer == 3:
    model1.add(layers.MaxPooling2D((2,2)))
    model1.add(layers.Conv2D(64,(k,k), activation='relu'))
    model1.add(layers.MaxPooling2D((2,2)))
    model1.add(layers.Conv2D(64,(k,k), activation='relu'))
    model1.add(layers.MaxPooling2D((2,2)))
    model1.add(layers.Conv2D(64,(k,k), activation='relu'))



  model1.add(layers.Flatten())
  model1.add(layers.Dense(64, activation = 'relu'))
  model1.add(layers.Dense(20))

  # Compile and Train Model
  model1.compile(optimizer = 'adam',
                loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
                metrics = ['accuracy'])

  # Create an early stopping callback
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=5)
  for batch in batch_size:
    history = model1.fit(x_train_norm, y_train, epochs = 16, batch_size = batch,
                        validation_data = (x_valid_norm, y_valid), callbacks=[early_stopping])

    train_acc = history.history['accuracy']
    valid_acc = history.history['val_accuracy']

    new_row = {'Number of Layers': layer, 'Batch Size' : batch, 'Kernel Size': k,
                  'Training Accuracy': train_acc[-1], 'Validation Accuracy':valid_acc[-1]}

    df.loc[len(df)] = new_row

    df

df

# c)c
df_regul = pd.DataFrame(columns=['Number of Layers', 'Batch Size', 'Kernel Size', 'L2 Regularization', 'Dropout Regularization',  'Training Accuracy', 'Validation Accuracy'])
df_regul.style.set_caption("Accuracy Overview for different Model Architectures")
k = 2
batch=32
epoch = 16


l2_list = [0, 0.1, 0.01]
do_list = [0, 0.3, 0.5]

for l2 in l2_list:
  for do in do_list:
    model_c = models.Sequential()
    model_c.add(layers.Conv2D(32, (k,k), activation='relu', input_shape = (32,32, 1)))
    model_c.add(layers.MaxPooling2D((2,2)))
    model_c.add(layers.Conv2D(64,(k,k), activation='relu', kernel_regularizer=regularizers.l2(l2)))
    model_c.add(layers.MaxPooling2D((2,2)))
    model_c.add(layers.Conv2D(64,(k,k), activation='relu', kernel_regularizer=regularizers.l2(l2)))
    model_c.add(layers.MaxPooling2D((2,2)))
    model_c.add(layers.Conv2D(64,(k,k), activation='relu', kernel_regularizer=regularizers.l2(l2)))
    model_c.add(layers.Flatten())
    model_c.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2)))
    model_c.add(layers.Dropout(do))
    model_c.add(layers.Dense(20))

    # Compile and Train Model
    model_c.compile(optimizer = 'adam',
                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
                  metrics = ['accuracy'])

    # Create an early stopping callback
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=5)

    history = model_c.fit(x_train_norm, y_train, epochs = epoch, batch_size = batch,
                        validation_data = (x_valid_norm, y_valid), callbacks=[early_stopping])

    train_acc = history.history['accuracy']
    valid_acc = history.history['val_accuracy']

    new_row = {'Number of Layers': 3, 'Batch Size' : batch, 'Kernel Size': k,
              'L2 Regularization': l2, 'Dropout Regularization': do,
                  'Training Accuracy': train_acc[-1], 'Validation Accuracy':valid_acc[-1]}

    df_regul.loc[len(df_regul)] = new_row

df_regul

# d)
k = 2
batch=32
epoch = 16
l2 = 0.0
do = 0.0

model_d = models.Sequential()
model_d.add(layers.Conv2D(32, (k,k), activation='relu', input_shape = (32,32, 1)))
model_d.add(layers.MaxPooling2D((2,2)))
model_d.add(layers.Conv2D(64,(k,k), activation='relu', kernel_regularizer=regularizers.l2(l2)))
model_d.add(layers.MaxPooling2D((2,2)))
model_d.add(layers.Conv2D(64,(k,k), activation='relu', kernel_regularizer=regularizers.l2(l2)))
model_d.add(layers.MaxPooling2D((2,2)))
model_d.add(layers.Conv2D(64,(k,k), activation='relu', kernel_regularizer=regularizers.l2(l2)))
model_d.add(layers.Flatten())
model_d.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2)))
model_d.add(layers.Dropout(do))
model_d.add(layers.Dense(20))

# Compile and Train Model
model_d.compile(optimizer = 'adam',
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy'])

# Create an early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=5)

history = model_d.fit(x_train_norm, y_train, epochs = epoch, batch_size = batch,
                    validation_data = (x_valid_norm, y_valid), callbacks=[early_stopping])

train_acc = history.history['accuracy']
valid_acc = history.history['val_accuracy']

# Plot Training Loss and Validation Loss
# Variables
x_batch = list(range(1,len(train_acc)+1, 1))


# Plots
plt.plot(train_acc, label = 'Training Accuracy')
plt.plot(valid_acc, label = 'Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Evolution of Training and Validation Accuracies during Training')
plt.legend()
plt.show()

# d)
# Applying whole Data set
train_data, test_data = tfd.cifar100.load_data(label_mode="coarse")
(x_train, y_train), (x_test, y_test) = train_data, test_data
x_train, x_test = np.mean(x_train, axis=3), np.mean(x_test, axis=3)

# Normalize data - Pixel intensity
x_train_norm = x_train/255
x_test_norm = x_test/255

# Final Training on whole Data Set
history = model_d.fit(x_train_norm, y_train, epochs = epoch,
                      batch_size = batch, callbacks=[early_stopping])

test_loss, test_acc = model_d.evaluate(x_test_norm, y_test, verbose=2)
print('Test accuracy:', test_acc)

# Generate predictions on the test set
y_pred = model_d.predict(x_test_norm)
y_pred_classes = np.argmax(y_pred, axis=1)

# Generate a confusion matrix
conf_mat = confusion_matrix(y_test, y_pred_classes)

# Plot the confusion matrix using seaborn
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# e)
# Load dataset
train_data, test_data = tfd.cifar100.load_data(label_mode="coarse")
(x_train, y_train), (x_test, y_test) = train_data, test_data
x_train, x_test = np.mean(x_train, axis=3), np.mean(x_test, axis=3)

# Load perturbed test set
with open('cifar20_perturb_test.pkl', 'rb') as f:
    model = pickle.load(f)
    glove_words =  set(model.keys())
# dict = pickle.load(open("cifar20_perturb_test.pkl", "rb"))
x_perturb, y_perturb = dict["x_perturb"], dict["y_perturb"]
x_perturb = np.mean(x_perturb, axis=3)

# Normalize data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_perturb = x_perturb.astype('float32') / 255.0


num_classes = 20
y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes)
y_perturb_encoded = tf.keras.utils.to_categorical(y_perturb, num_classes)

x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(
    x_train, y_train_encoded, test_size=0.2, stratify=y_train_encoded, random_state=42)

config = {
    'l2_weight': 0.0,
    'dropout_rate': 0.0}

def create_final_model(num_classes, l2_weight, dropout_rate):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(32, 32, 1)),
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(32, (5, 5), activation='relu'),
        tf.keras.layers.Conv2D(32, (5, 5), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_weight)),
        tf.keras.layers.Dropout(dropout_rate),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

    return model
final_model = create_final_model(num_classes, config['l2_weight'], config['dropout_rate'])
history_final = final_model.fit(x_train, y_train_encoded, epochs=1, batch_size=32, validation_data=(x_val_split, y_val_split), verbose=1)

def evaluate_on_perturbed_test_set(model, x_test, y_test_encoded, x_perturb, y_perturb_encoded):
    # Evaluate the model on the test set and perturbed test set
    test_loss, test_accuracy = model.evaluate(x_test, y_test_encoded)
    perturb_test_loss, perturb_test_accuracy = model.evaluate(x_perturb, y_perturb_encoded)
    print(f"Test Accuracy: {test_accuracy}")
    print(f"Perturbed Test Accuracy: {perturb_test_accuracy}")

# Approach 1: Regularization and Dropout
def apply_regularization_and_dropout(model, l2_weight=0.001, dropout_rate=0.3):
    for layer in model.layers:
        if isinstance(layer, tf.keras.layers.Dense) or isinstance(layer, tf.keras.layers.Conv2D):
            layer.kernel_regularizer = tf.keras.regularizers.l2(l2_weight)
        if isinstance(layer, tf.keras.layers.Dropout):
            layer.rate = dropout_rate
    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Approach 2: Batch Normalization
def apply_batch_normalization(model):
    modified_model = tf.keras.models.Sequential()
    for layer in model.layers:
        modified_model.add(layer)
        if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):
            modified_model.add(tf.keras.layers.BatchNormalization())
    modified_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
    return modified_model

# Approach 3: Data Augmentation
def apply_data_augmentation(x_train, y_train):
    data_augmentation = tf.keras.Sequential([
        tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),
        tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),
    ])
    augmented_x_train = data_augmentation(x_train)
    return augmented_x_train, y_train

# Apply different approaches and evaluate their performance
approaches = {
    "Baseline": final_model,
    "Regularization and Dropout": apply_regularization_and_dropout(final_model, l2_weight=0.001, dropout_rate=0.3),
    "Batch Normalization": apply_batch_normalization(final_model),
}

for approach_name, modified_model in approaches.items():
    print(f"Evaluating Model with {approach_name}")
    modified_model.fit(x_train_split, y_train_split, epochs=1, batch_size=32, validation_data=(x_val_split, y_val_split), verbose=0)
    evaluate_on_perturbed_test_set(modified_model, x_test, y_test_encoded, x_perturb, y_perturb_encoded)

